{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09b8b956-dc92-4cb6-a366-bb534a15736f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "\n",
    "## Módulo: ED-NA-001 - Extração de Dados I\n",
    "<br>\n",
    "\n",
    "## Aula 5 - Exercício 1 - Parte 1\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff8c8c64-7811-4deb-be3b-7273774ba0ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Desenvolver um pipeline de dados com as seguintes etapas:\n",
    "> #### 1: Extrair os dados da API Random User Generator (https://randomuser.me/documentation) e selecionar apenas os campos de nome, e-mail, país\te telefone do usuário aleatório;\n",
    "> #### 2: Enviar os dados extraidos para um tópico chamado \"topico_API\" no Kafka a partir de um produtor;\n",
    "> #### 3: Consumir os dados do tópico \"topico_API\" com o Spark Streaming e armazenar os resultados e um tópico chamado \"resultados_API\" no Kafka;\n",
    "> #### 4: Consumir os dados do tópico \"resultados_API\" com o Spark Streaming e armazenar os resultados em arquivos Parquet em uma pasta no DBFS."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Aula_5_Exercicio_1_Parte_1_Kafka_e_Spark_Streaming",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
