{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a953fa75-3330-47f2-be3c-9766b2295840",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<div style=\"text-align: left;\">\n",
    "\n",
    "## Módulo: ED-NA-001 - Extração de Dados I\n",
    "<br>\n",
    "\n",
    "## Aula 1 - Exercício 1\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf2a9d95-a821-4704-8bc9-02ad012a5253",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Desenvolver em grupo um ETL e um ELT a partir da extração de dados do arquivo disponível no Kaggle:\n",
    "https://www.kaggle.com/datasets/kapturovalexander/spotify-data-from-pyspark-course\n",
    "\n",
    "#### \"Cada linha no conjunto de dados corresponde a uma faixa musical, com variáveis como o título, artista e ano localizadas em suas respectivas colunas. Além das variáveis fundamentais, elementos musicais de cada faixa, como o ritmo, a capacidade de dança e a tonalidade, também foram extraídos; o algoritmo para esses valores foi gerado pelo Spotify com base em uma série de parâmetros técnicos.\"\n",
    "\n",
    "#### Essa é a descrição e o tipo de cada coluna do arquivo:\n",
    "\n",
    "> #### 01. id: str, identificador da faixa.\n",
    "> #### 02. nome: str, nome da faixa.\n",
    "> #### 03. artistas: str, artistas da faixa.\n",
    "> #### 04. duration_ms: float, duração da faixa em milissegundos.\n",
    "> #### 05. release_date: data, data de lançamento da faixa.\n",
    "> #### 06. year: int, ano de lançamento da faixa.\n",
    "> #### 07. acousticness: float, medida de acústica da faixa.\n",
    "> #### 08. danceability: float, medida de capacidade de dança da faixa.\n",
    "> #### 09. energy: float, medida de energia da faixa.\n",
    "> #### 10. instrumentalness: float, medida de elementos instrumentais na faixa.\n",
    "> #### 11. liveness: float, medida de vivacidade da faixa.\n",
    "> #### 12. loudness: float, volume da faixa.\n",
    "> #### 13. speechiness: float, medida de fala na faixa.\n",
    "> #### 14. tempo: float, ritmo da faixa.\n",
    "> #### 15. valence: float, medida de valência (positividade) da faixa.\n",
    "> #### 16. mode: int, modo da faixa (maior ou menor).\n",
    "> #### 17. key: int, tonalidade da faixa.\n",
    "> #### 18. popularity: int, pontuação de popularidade da faixa.\n",
    "> #### 19. explicit: int, indicação da presença de conteúdo explícito (explícito ou implícito)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "908f13c2-a942-429a-b238-462a4a1519df",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Os dados transformados precisam responder as seguintes perguntas:\n",
    "> #### 1. As músicas mais populares nos últimos 10 anos;\n",
    "> #### 2. A quantidade de músicas lançadas por artista e ano de lançamento;\n",
    "> #### 3. Os artistas com a maior média de popularidade de músicas nos últimos 5 anos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\pandas\\utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169909, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>artists</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>release_date</th>\n",
       "      <th>year</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>mode</th>\n",
       "      <th>key</th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6KbQ3uYMLKb5jDxLF7wYDD</td>\n",
       "      <td>Singende Bataillone 1. Teil</td>\n",
       "      <td>['Carl Woitschach']</td>\n",
       "      <td>158648</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.708</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.563</td>\n",
       "      <td>0.151</td>\n",
       "      <td>-12.428</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>118.469</td>\n",
       "      <td>0.779</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6KuQTIu1KoTTkLXKrwlLPV</td>\n",
       "      <td>Fantasiestücke, Op. 111: Più tosto lento</td>\n",
       "      <td>['Robert Schumann', 'Vladimir Horowitz']</td>\n",
       "      <td>282133</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>-28.454</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>83.972</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6L63VW0PibdM1HDSBoqnoM</td>\n",
       "      <td>Chapter 1.18 - Zamek kaniowski</td>\n",
       "      <td>['Seweryn Goszczyński']</td>\n",
       "      <td>104300</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119</td>\n",
       "      <td>-19.924</td>\n",
       "      <td>0.929</td>\n",
       "      <td>107.177</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6M94FkXd15sOAOQYRnWPN8</td>\n",
       "      <td>Bebamos Juntos - Instrumental (Remasterizado)</td>\n",
       "      <td>['Francisco Canaro']</td>\n",
       "      <td>180760</td>\n",
       "      <td>9/25/28</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.111</td>\n",
       "      <td>-14.734</td>\n",
       "      <td>0.0926</td>\n",
       "      <td>108.003</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6N6tiFZ9vLTSOIxkj8qKrd</td>\n",
       "      <td>Polonaise-Fantaisie in A-Flat Major, Op. 61</td>\n",
       "      <td>['Frédéric Chopin', 'Vladimir Horowitz']</td>\n",
       "      <td>687733</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.098</td>\n",
       "      <td>-16.829</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>62.149</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6NxAf7M8DNHOBTmEd3JSO5</td>\n",
       "      <td>Scherzo a capriccio: Presto</td>\n",
       "      <td>['Felix Mendelssohn', 'Vladimir Horowitz']</td>\n",
       "      <td>352600</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>-19.242</td>\n",
       "      <td>0.0593</td>\n",
       "      <td>63.521</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6O0puPuyrxPjDTHDUgsWI7</td>\n",
       "      <td>Valse oubliée No. 1 in F-Sharp Major, S. 215/1</td>\n",
       "      <td>['Franz Liszt', 'Vladimir Horowitz']</td>\n",
       "      <td>136627</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.956</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.197</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>-17.226</td>\n",
       "      <td>0.04</td>\n",
       "      <td>80.495</td>\n",
       "      <td>0.305</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6OJjveoYwJdIt76y0Pxpxw</td>\n",
       "      <td>Per aspera ad astra</td>\n",
       "      <td>['Carl Woitschach']</td>\n",
       "      <td>153967</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.988</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.421</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.105</td>\n",
       "      <td>-9.878</td>\n",
       "      <td>0.0474</td>\n",
       "      <td>123.31</td>\n",
       "      <td>0.857</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6OaJ8Bh7lsBeYoBmwmo2nh</td>\n",
       "      <td>Moneda Corriente - Remasterizado</td>\n",
       "      <td>['Francisco Canaro', 'Charlo']</td>\n",
       "      <td>162493</td>\n",
       "      <td>10/3/28</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.337</td>\n",
       "      <td>-9.801</td>\n",
       "      <td>0.127</td>\n",
       "      <td>119.833</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6PrZexNb16cabXR8Q418Xc</td>\n",
       "      <td>Chapter 1.3 - Zamek kaniowski</td>\n",
       "      <td>['Seweryn Goszczyński']</td>\n",
       "      <td>111600</td>\n",
       "      <td>1928</td>\n",
       "      <td>1928</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-20.119</td>\n",
       "      <td>0.954</td>\n",
       "      <td>81.249</td>\n",
       "      <td>0.759</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                            name                                     artists duration_ms release_date  year acousticness danceability  energy instrumentalness liveness loudness speechiness    tempo valence mode key popularity explicit\n",
       "0  6KbQ3uYMLKb5jDxLF7wYDD                     Singende Bataillone 1. Teil                         ['Carl Woitschach']      158648         1928  1928        0.995        0.708   0.195            0.563    0.151  -12.428      0.0506  118.469   0.779    1  10          0        0\n",
       "1  6KuQTIu1KoTTkLXKrwlLPV        Fantasiestücke, Op. 111: Più tosto lento    ['Robert Schumann', 'Vladimir Horowitz']      282133         1928  1928        0.994        0.379  0.0135            0.901   0.0763  -28.454      0.0462   83.972  0.0767    1   8          0        0\n",
       "2  6L63VW0PibdM1HDSBoqnoM                  Chapter 1.18 - Zamek kaniowski                     ['Seweryn Goszczyński']      104300         1928  1928        0.604        0.749    0.22                0    0.119  -19.924       0.929  107.177    0.88    0   5          0        0\n",
       "3  6M94FkXd15sOAOQYRnWPN8   Bebamos Juntos - Instrumental (Remasterizado)                        ['Francisco Canaro']      180760      9/25/28  1928        0.995        0.781    0.13            0.887    0.111  -14.734      0.0926  108.003    0.72    0   1          0        0\n",
       "4  6N6tiFZ9vLTSOIxkj8qKrd     Polonaise-Fantaisie in A-Flat Major, Op. 61    ['Frédéric Chopin', 'Vladimir Horowitz']      687733         1928  1928         0.99         0.21   0.204            0.908    0.098  -16.829      0.0424   62.149  0.0693    1  11          1        0\n",
       "5  6NxAf7M8DNHOBTmEd3JSO5                     Scherzo a capriccio: Presto  ['Felix Mendelssohn', 'Vladimir Horowitz']      352600         1928  1928        0.995        0.424    0.12            0.911   0.0915  -19.242      0.0593   63.521   0.266    0   6          0        0\n",
       "6  6O0puPuyrxPjDTHDUgsWI7  Valse oubliée No. 1 in F-Sharp Major, S. 215/1        ['Franz Liszt', 'Vladimir Horowitz']      136627         1928  1928        0.956        0.444   0.197            0.435   0.0744  -17.226        0.04   80.495   0.305    1  11          0        0\n",
       "7  6OJjveoYwJdIt76y0Pxpxw                             Per aspera ad astra                         ['Carl Woitschach']      153967         1928  1928        0.988        0.555   0.421            0.836    0.105   -9.878      0.0474   123.31   0.857    1   1          0        0\n",
       "8  6OaJ8Bh7lsBeYoBmwmo2nh                Moneda Corriente - Remasterizado              ['Francisco Canaro', 'Charlo']      162493      10/3/28  1928        0.995        0.683   0.207            0.206    0.337   -9.801       0.127  119.833   0.493    0   9          0        0\n",
       "9  6PrZexNb16cabXR8Q418Xc                   Chapter 1.3 - Zamek kaniowski                     ['Seweryn Goszczyński']      111600         1928  1928        0.846        0.674   0.205                0     0.17  -20.119       0.954   81.249   0.759    1   9          0        0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arq_leitura = \"H:\\Outros computadores\\Meu laptop\\Trilha de Formação Engenharia de Dados\\Módulo 4 - Extração de Dados\\Repositório Exercícios\\Data\\spotify-data.csv\"\n",
    "\n",
    "#index_col_name = [\"id\"]\n",
    "\n",
    "df = ps.read_csv(arq_leitura) #index_col=index_col_name)\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. As músicas mais populares nos últimos 10 anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168932, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>artists</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>release_date</th>\n",
       "      <th>year</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>loudness</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>tempo</th>\n",
       "      <th>valence</th>\n",
       "      <th>mode</th>\n",
       "      <th>key</th>\n",
       "      <th>popularity</th>\n",
       "      <th>explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7ytR5pFWmSjzHJIeQkgog4</td>\n",
       "      <td>ROCKSTAR (feat. Roddy Ricch)</td>\n",
       "      <td>['DaBaby', 'Roddy Ricch']</td>\n",
       "      <td>181733</td>\n",
       "      <td>4/17/20</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101</td>\n",
       "      <td>-7.956</td>\n",
       "      <td>0.164</td>\n",
       "      <td>89.977</td>\n",
       "      <td>0.497</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7eJMfftS33KTjuF7lTsMCx</td>\n",
       "      <td>death bed (coffee for your head) (feat. beabad...</td>\n",
       "      <td>['Powfu', 'beabadoobee']</td>\n",
       "      <td>173333</td>\n",
       "      <td>2/8/20</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.726</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0</td>\n",
       "      <td>0.696</td>\n",
       "      <td>-8.765</td>\n",
       "      <td>0.135</td>\n",
       "      <td>144.026</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39Yp9wwQiSRIDOvrVg7mbk</td>\n",
       "      <td>THE SCOTTS</td>\n",
       "      <td>['THE SCOTTS', 'Travis Scott', 'Kid Cudi']</td>\n",
       "      <td>165978</td>\n",
       "      <td>4/24/20</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.233</td>\n",
       "      <td>0.716</td>\n",
       "      <td>0.537</td>\n",
       "      <td>0</td>\n",
       "      <td>0.157</td>\n",
       "      <td>-7.648</td>\n",
       "      <td>0.0514</td>\n",
       "      <td>129.979</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0nbXyq5TXYPCO7pr3N8S4I</td>\n",
       "      <td>The Box</td>\n",
       "      <td>['Roddy Ricch']</td>\n",
       "      <td>196653</td>\n",
       "      <td>12/6/19</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.586</td>\n",
       "      <td>0</td>\n",
       "      <td>0.79</td>\n",
       "      <td>-6.687</td>\n",
       "      <td>0.0559</td>\n",
       "      <td>116.971</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4nK5YrxbMGZstTLbvj6Gxw</td>\n",
       "      <td>Supalonely</td>\n",
       "      <td>['BENEE', 'Gus Dapperton']</td>\n",
       "      <td>223480</td>\n",
       "      <td>11/15/19</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.863</td>\n",
       "      <td>0.631</td>\n",
       "      <td>3.04E-05</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-4.689</td>\n",
       "      <td>0.0534</td>\n",
       "      <td>128.977</td>\n",
       "      <td>0.817</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62aP9fBQKYKxi7PDXwcUAS</td>\n",
       "      <td>ily (i love you baby) (feat. Emilee)</td>\n",
       "      <td>['Surf Mesa', 'Emilee']</td>\n",
       "      <td>176547</td>\n",
       "      <td>11/26/19</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.0686</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.00188</td>\n",
       "      <td>0.393</td>\n",
       "      <td>-7.567</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>112.05</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>127QTOFJsJQp5LbJbu3A1y</td>\n",
       "      <td>Toosie Slide</td>\n",
       "      <td>['Drake']</td>\n",
       "      <td>247059</td>\n",
       "      <td>4/3/20</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.454</td>\n",
       "      <td>6.15E-06</td>\n",
       "      <td>0.114</td>\n",
       "      <td>-9.75</td>\n",
       "      <td>0.201</td>\n",
       "      <td>81.618</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2XU0oxnq2qxCpomAAuJY8K</td>\n",
       "      <td>Dance Monkey</td>\n",
       "      <td>['Tones And I']</td>\n",
       "      <td>209438</td>\n",
       "      <td>10/17/19</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.588</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>0.149</td>\n",
       "      <td>-6.4</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>98.027</td>\n",
       "      <td>0.513</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5RqR4ZCCKJDcBLIn4sih9l</td>\n",
       "      <td>Party Girl</td>\n",
       "      <td>['StaySolidRocky']</td>\n",
       "      <td>147800</td>\n",
       "      <td>4/21/20</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0996</td>\n",
       "      <td>-9.966</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>130.022</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4NhDYoQTYCdWHTvlbGVgwo</td>\n",
       "      <td>GOOBA</td>\n",
       "      <td>['6ix9ine']</td>\n",
       "      <td>132303</td>\n",
       "      <td>5/8/20</td>\n",
       "      <td>2020</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0</td>\n",
       "      <td>0.251</td>\n",
       "      <td>-5.688</td>\n",
       "      <td>0.341</td>\n",
       "      <td>178.462</td>\n",
       "      <td>0.393</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       id                                                  name                                     artists duration_ms release_date  year acousticness danceability energy instrumentalness liveness loudness speechiness    tempo valence mode key popularity explicit\n",
       "0  7ytR5pFWmSjzHJIeQkgog4                          ROCKSTAR (feat. Roddy Ricch)                   ['DaBaby', 'Roddy Ricch']      181733      4/17/20  2020        0.247        0.746   0.69                0    0.101   -7.956       0.164   89.977   0.497    1  11         99        1\n",
       "1  7eJMfftS33KTjuF7lTsMCx  death bed (coffee for your head) (feat. beabadoobee)                    ['Powfu', 'beabadoobee']      173333       2/8/20  2020        0.731        0.726  0.431                0    0.696   -8.765       0.135  144.026   0.348    0   8         97        0\n",
       "2  39Yp9wwQiSRIDOvrVg7mbk                                            THE SCOTTS  ['THE SCOTTS', 'Travis Scott', 'Kid Cudi']      165978      4/24/20  2020        0.233        0.716  0.537                0    0.157   -7.648      0.0514  129.979    0.28    0   0         96        1\n",
       "3  0nbXyq5TXYPCO7pr3N8S4I                                               The Box                             ['Roddy Ricch']      196653      12/6/19  2019        0.104        0.896  0.586                0     0.79   -6.687      0.0559  116.971   0.642    0  10         95        1\n",
       "4  4nK5YrxbMGZstTLbvj6Gxw                                            Supalonely                  ['BENEE', 'Gus Dapperton']      223480     11/15/19  2019        0.305        0.863  0.631         3.04E-05    0.123   -4.689      0.0534  128.977   0.817    1   7         95        1\n",
       "5  62aP9fBQKYKxi7PDXwcUAS                  ily (i love you baby) (feat. Emilee)                     ['Surf Mesa', 'Emilee']      176547     11/26/19  2019       0.0686        0.674  0.774          0.00188    0.393   -7.567      0.0892   112.05    0.33    0  11         95        0\n",
       "6  127QTOFJsJQp5LbJbu3A1y                                          Toosie Slide                                   ['Drake']      247059       4/3/20  2020        0.321        0.834  0.454         6.15E-06    0.114    -9.75       0.201   81.618   0.837    0   1         95        1\n",
       "7  2XU0oxnq2qxCpomAAuJY8K                                          Dance Monkey                             ['Tones And I']      209438     10/17/19  2019        0.692        0.824  0.588         0.000104    0.149     -6.4      0.0924   98.027   0.513    0   6         94        0\n",
       "8  5RqR4ZCCKJDcBLIn4sih9l                                            Party Girl                          ['StaySolidRocky']      147800      4/21/20  2020        0.749        0.728  0.431                0   0.0996   -9.966      0.0622  130.022   0.629    0   6         94        0\n",
       "9  4NhDYoQTYCdWHTvlbGVgwo                                                 GOOBA                                 ['6ix9ine']      132303       5/8/20  2020       0.0264        0.611  0.688                0    0.251   -5.688       0.341  178.462   0.393    1   1         94        1"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A base de dados termina em 2020\n",
    "mais_populares_ultimos_10_anos = df[(df[\"year\"]>=1900) & (df[\"year\"]<=2020)]\n",
    "\n",
    "\n",
    "mais_populares_ultimos_10_anos = mais_populares_ultimos_10_anos.sort_values(by=['popularity'], ascending=False)\n",
    "\n",
    "mais_populares_ultimos_10_anos = mais_populares_ultimos_10_anos.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(mais_populares_ultimos_10_anos.shape)\n",
    "mais_populares_ultimos_10_anos.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 2. A quantidade de músicas lançadas por artista e ano de lançamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>artists</th>\n",
       "      <th>qtd_musicas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020</td>\n",
       "      <td>['Lil Uzi Vert']</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020</td>\n",
       "      <td>['The Weeknd']</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020</td>\n",
       "      <td>['Lil Baby']</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020</td>\n",
       "      <td>['YoungBoy Never Broke Again']</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>['Lauv']</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020</td>\n",
       "      <td>['Future']</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020</td>\n",
       "      <td>['Dua Lipa']</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020</td>\n",
       "      <td>['The 1975']</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020</td>\n",
       "      <td>['Rod Wave']</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020</td>\n",
       "      <td>['BTS']</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year                         artists  qtd_musicas\n",
       "0  2020                ['Lil Uzi Vert']           37\n",
       "1  2020                  ['The Weeknd']           24\n",
       "2  2020                    ['Lil Baby']           23\n",
       "3  2020  ['YoungBoy Never Broke Again']           23\n",
       "4  2020                        ['Lauv']           20\n",
       "5  2020                      ['Future']           15\n",
       "6  2020                    ['Dua Lipa']           15\n",
       "7  2020                    ['The 1975']           15\n",
       "8  2020                    ['Rod Wave']           14\n",
       "9  2020                         ['BTS']           14"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "musicas_lancadas = df[(df[\"year\"]>=1900) & (df[\"year\"]<=2020)]\n",
    "\n",
    "musicas_lancadas = musicas_lancadas.groupby(['year', 'artists']).agg(qtd_musicas=('id', 'count'))\n",
    "\n",
    "musicas_lancadas = musicas_lancadas.reset_index(drop=False)\n",
    "\n",
    "musicas_lancadas = musicas_lancadas.sort_values(by=['year', 'qtd_musicas'], ascending=False)\n",
    "\n",
    "musicas_lancadas = musicas_lancadas.reset_index(drop=True)\n",
    "\n",
    "musicas_lancadas.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # 3. Os artistas com a maior média de popularidade de músicas nos últimos 5 anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artists</th>\n",
       "      <th>media_popularidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>['Powfu', 'beabadoobee']</td>\n",
       "      <td>97.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3073</th>\n",
       "      <td>['BENEE', 'Gus Dapperton']</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3085</th>\n",
       "      <td>['Surf Mesa', 'Emilee']</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>['Ariana Grande', 'Justin Bieber']</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>['StaySolidRocky']</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>['KAROL G', 'Nicki Minaj']</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3276</th>\n",
       "      <td>['Topic', 'A7S']</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>['Bad Bunny', 'Jowell &amp; Randy', 'Nengo Flow']</td>\n",
       "      <td>92.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>['Marshmello', 'Halsey']</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2585</th>\n",
       "      <td>['Lady Gaga', 'Ariana Grande']</td>\n",
       "      <td>90.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            artists  media_popularidade\n",
       "2565                       ['Powfu', 'beabadoobee']                97.0\n",
       "3073                     ['BENEE', 'Gus Dapperton']                95.0\n",
       "3085                        ['Surf Mesa', 'Emilee']                95.0\n",
       "2639             ['Ariana Grande', 'Justin Bieber']                94.0\n",
       "3379                             ['StaySolidRocky']                94.0\n",
       "3087                     ['KAROL G', 'Nicki Minaj']                92.0\n",
       "3276                               ['Topic', 'A7S']                92.0\n",
       "3281  ['Bad Bunny', 'Jowell & Randy', 'Nengo Flow']                92.0\n",
       "2524                       ['Marshmello', 'Halsey']                91.0\n",
       "2585                 ['Lady Gaga', 'Ariana Grande']                90.5"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artistas_maior_popularidade = df[(df['year']>2015) & (df['year']<=2020)]\n",
    "\n",
    "artistas_maior_popularidade = artistas_maior_popularidade.groupby(['artists']).agg(media_popularidade=('popularity', 'avg'))\n",
    "\n",
    "artistas_maior_popularidade = artistas_maior_popularidade.reset_index(drop=False)\n",
    "\n",
    "artistas_maior_popularidade = artistas_maior_popularidade.sort_values(by=['media_popularidade'], ascending=False)\n",
    "\n",
    "artistas_maior_popularidade.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Armazenando os dados transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o27088.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 501.0 failed 1 times, most recent failure: Lost task 2.0 in stage 501.0 (TID 1587) (COMP22 executor driver): java.io.IOException: Mkdirs failed to create file:/H:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios/Data_Transformed\resultado_1.parquet/_temporary/0/_temporary/attempt_202310101245147125685854529398510_0501_m_000002_1587 (exists=false, cwd=file:/h:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/H:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios/Data_Transformed\resultado_1.parquet/_temporary/0/_temporary/attempt_202310101245147125685854529398510_0501_m_000002_1587 (exists=false, cwd=file:/h:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mh:\\Outros computadores\\Meu laptop\\Trilha de Formação Engenharia de Dados\\Módulo 4 - Extração de Dados\\Repositório Exercícios\\Aula_1_Exercicio_1_Desenvolvimento_de_um_ETL_e_ELT.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Outros%20computadores/Meu%20laptop/Trilha%20de%20Forma%C3%A7%C3%A3o%20Engenharia%20de%20Dados/M%C3%B3dulo%204%20-%20Extra%C3%A7%C3%A3o%20de%20Dados/Reposit%C3%B3rio%20Exerc%C3%ADcios/Aula_1_Exercicio_1_Desenvolvimento_de_um_ETL_e_ELT.ipynb#Y121sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Resultado 1\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Outros%20computadores/Meu%20laptop/Trilha%20de%20Forma%C3%A7%C3%A3o%20Engenharia%20de%20Dados/M%C3%B3dulo%204%20-%20Extra%C3%A7%C3%A3o%20de%20Dados/Reposit%C3%B3rio%20Exerc%C3%ADcios/Aula_1_Exercicio_1_Desenvolvimento_de_um_ETL_e_ELT.ipynb#Y121sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m arq_escrita \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOutros computadores\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMeu laptop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTrilha de Formação Engenharia de Dados\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMódulo 4 - Extração de Dados\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRepositório Exercícios\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mData_Transformed\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mesultado_1.parquet\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/h%3A/Outros%20computadores/Meu%20laptop/Trilha%20de%20Forma%C3%A7%C3%A3o%20Engenharia%20de%20Dados/M%C3%B3dulo%204%20-%20Extra%C3%A7%C3%A3o%20de%20Dados/Reposit%C3%B3rio%20Exerc%C3%ADcios/Aula_1_Exercicio_1_Desenvolvimento_de_um_ETL_e_ELT.ipynb#Y121sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m mais_populares_ultimos_10_anos\u001b[39m.\u001b[39;49mto_parquet(arq_escrita, index_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mto_parquet\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Outros%20computadores/Meu%20laptop/Trilha%20de%20Forma%C3%A7%C3%A3o%20Engenharia%20de%20Dados/M%C3%B3dulo%204%20-%20Extra%C3%A7%C3%A3o%20de%20Dados/Reposit%C3%B3rio%20Exerc%C3%ADcios/Aula_1_Exercicio_1_Desenvolvimento_de_um_ETL_e_ELT.ipynb#Y121sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Resultado 2\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/h%3A/Outros%20computadores/Meu%20laptop/Trilha%20de%20Forma%C3%A7%C3%A3o%20Engenharia%20de%20Dados/M%C3%B3dulo%204%20-%20Extra%C3%A7%C3%A3o%20de%20Dados/Reposit%C3%B3rio%20Exerc%C3%ADcios/Aula_1_Exercicio_1_Desenvolvimento_de_um_ETL_e_ELT.ipynb#Y121sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m arq_escrita \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mOutros computadores\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMeu laptop\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mTrilha de Formação Engenharia de Dados\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMódulo 4 - Extração de Dados\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mRepositório Exercícios\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mData_Transformed\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39mesultado_2.parquet\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\pandas\\frame.py:5282\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, mode, partition_cols, compression, index_col, **options)\u001b[0m\n\u001b[0;32m   5280\u001b[0m \u001b[39mif\u001b[39;00m compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   5281\u001b[0m     builder\u001b[39m.\u001b[39moption(\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m, compression)\n\u001b[1;32m-> 5282\u001b[0m builder\u001b[39m.\u001b[39;49moptions(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptions)\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mparquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\sql\\readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m   1462\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o27088.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 501.0 failed 1 times, most recent failure: Lost task 2.0 in stage 501.0 (TID 1587) (COMP22 executor driver): java.io.IOException: Mkdirs failed to create file:/H:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios/Data_Transformed\resultado_1.parquet/_temporary/0/_temporary/attempt_202310101245147125685854529398510_0501_m_000002_1587 (exists=false, cwd=file:/h:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:402)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:374)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:833)\r\nCaused by: java.io.IOException: Mkdirs failed to create file:/H:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios/Data_Transformed\resultado_1.parquet/_temporary/0/_temporary/attempt_202310101245147125685854529398510_0501_m_000002_1587 (exists=false, cwd=file:/h:/Outros computadores/Meu laptop/Trilha de Formação Engenharia de Dados/Módulo 4 - Extração de Dados/Repositório Exercícios)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\r\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "# Resultado 1\n",
    "arq_escrita = \"H:\\Outros computadores\\Meu laptop\\Trilha de Formação Engenharia de Dados\\Módulo 4 - Extração de Dados\\Repositório Exercícios\\Data_Transformed\\resultado_1.parquet\"\n",
    "mais_populares_ultimos_10_anos.to_parquet(arq_escrita, index_col='to_parquet')\n",
    "\n",
    "# Resultado 2\n",
    "arq_escrita = \"H:\\Outros computadores\\Meu laptop\\Trilha de Formação Engenharia de Dados\\Módulo 4 - Extração de Dados\\Repositório Exercícios\\Data_Transformed\\resultado_2.parquet\"\n",
    "musicas_lancadas.to_parquet(arq_escrita, index_col='to_parquet')\n",
    "\n",
    "# Resultado \n",
    "arq_escrita = \"H:\\Outros computadores\\Meu laptop\\Trilha de Formação Engenharia de Dados\\Módulo 4 - Extração de Dados\\Repositório Exercícios\\Data_Transformed\\resultado_3.parquet\"\n",
    "artistas_maior_popularidade.to_parquet(arq_escrita, index_col='to_parquet')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 368097242019978,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Aula_1_Exercicio_1_Desenvolvimento_de_um_ETL_e_ELT",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
